{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11690984,"sourceType":"datasetVersion","datasetId":7337889},{"sourceId":11691172,"sourceType":"datasetVersion","datasetId":7338011},{"sourceId":11691205,"sourceType":"datasetVersion","datasetId":7338034},{"sourceId":11691213,"sourceType":"datasetVersion","datasetId":7338041}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qq peft==0.6.0\n!pip install -qq bitsandbytes==0.41.1\n!pip install -qq accelerate==0.24.1\n!pip install -qq transformers==4.35.0\n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/uttils-xla/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:06:35.983188Z","iopub.execute_input":"2025-05-05T21:06:35.984079Z","iopub.status.idle":"2025-05-05T21:09:08.899398Z","shell.execute_reply.started":"2025-05-05T21:06:35.984033Z","shell.execute_reply":"2025-05-05T21:09:08.894175Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndiffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.21.0 requires torch==2.6.0, but you have torch 2.1.2+cpu which is incompatible.\ntorchaudio 2.6.0 requires torch==2.6.0, but you have torch 2.1.2+cpu which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall bitsandbytes -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:16:44.870881Z","iopub.execute_input":"2025-05-05T21:16:44.871275Z","iopub.status.idle":"2025-05-05T21:16:45.533732Z","shell.execute_reply.started":"2025-05-05T21:16:44.871242Z","shell.execute_reply":"2025-05-05T21:16:45.527832Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport gc #Recolector de basura en memoria\nimport re #Buscar patrones en textos\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:16:47.725467Z","iopub.execute_input":"2025-05-05T21:16:47.726579Z","iopub.status.idle":"2025-05-05T21:16:47.740390Z","shell.execute_reply.started":"2025-05-05T21:16:47.726553Z","shell.execute_reply":"2025-05-05T21:16:47.736029Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:225: UserWarning: XLA_USE_SPMD is being deprecated. Use torch_xla.runtime.use_spmd() without setting XLA_USE_SPMD env-var.\n  warnings.warn(\"XLA_USE_SPMD is being deprecated. \"\n","output_type":"stream"},{"name":"stdout","text":"Torch Version: 2.1.2+cpu\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"**Configs**","metadata":{}},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 16\n    DROPOUT = 0.05\n    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n    SEED = 2024\n    MAX_LENGTH = 1024\n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5\n    NUM_LABELS = 3\n    LORA_RANK = 4\n    LORA_ALPHA = 8\n    LORA_MODULES = ['o_proj', 'v_proj']\n\nDEVICE = xm.xla_device()","metadata":{"_uuid":"255b3f7b-f310-40e7-a054-d7f065b1b359","_cell_guid":"00e24eaa-b07c-476c-b805-1c7f62315167","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-05T21:16:52.822884Z","iopub.execute_input":"2025-05-05T21:16:52.823224Z","iopub.status.idle":"2025-05-05T21:16:52.833232Z","shell.execute_reply.started":"2025-05-05T21:16:52.823196Z","shell.execute_reply":"2025-05-05T21:16:52.829621Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility\"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n\n    xm.set_rng_state(seed, device=xm.xla_device())\n\nset_seeds(seed=CFG.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:22:25.082004Z","iopub.execute_input":"2025-05-05T21:22:25.082288Z","iopub.status.idle":"2025-05-05T21:22:25.094499Z","shell.execute_reply.started":"2025-05-05T21:22:25.082263Z","shell.execute_reply":"2025-05-05T21:22:25.089878Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"**Tokenizer**","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n#Save tokenizer to load offline during inference\ntokenizer.save_pretrained('tokenizer')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:25:38.877852Z","iopub.execute_input":"2025-05-05T21:25:38.878229Z","iopub.status.idle":"2025-05-05T21:25:39.143904Z","shell.execute_reply.started":"2025-05-05T21:25:38.878201Z","shell.execute_reply":"2025-05-05T21:25:39.139072Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:718\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    720\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:550\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    549\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 550\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/utils/hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/llama-3/transformers/8b-chat-hf/1'. Use `repo_type` argument if needed."],"ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/llama-3/transformers/8b-chat-hf/1'. Use `repo_type` argument if needed.","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"#Utility function giving token lenght\ndef get_token_lengths(texts):\n    #tokenize and receive input_ids for reach text\n    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n    return [len(t) for t in input_ids]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:56:17.607040Z","iopub.execute_input":"2025-05-05T21:56:17.607398Z","iopub.status.idle":"2025-05-05T21:56:17.618621Z","shell.execute_reply.started":"2025-05-05T21:56:17.607372Z","shell.execute_reply":"2025-05-05T21:56:17.613242Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"**Prepare train**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n#Drop null for training\nindexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nprint(f'Total {len(indexes)} Null values rows dropped ')\nprint('Total train samples:', len(train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T22:18:02.713494Z","iopub.execute_input":"2025-05-05T22:18:02.713770Z","iopub.status.idle":"2025-05-05T22:18:04.887659Z","shell.execute_reply.started":"2025-05-05T22:18:02.713746Z","shell.execute_reply":"2025-05-05T22:18:04.883137Z"}},"outputs":[{"name":"stdout","text":"Total 19 Null values rows dropped \nTotal train samples: 57458\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T22:19:18.146828Z","iopub.execute_input":"2025-05-05T22:19:18.147150Z","iopub.status.idle":"2025-05-05T22:19:18.182133Z","shell.execute_reply.started":"2025-05-05T22:19:18.147122Z","shell.execute_reply":"2025-05-05T22:19:18.178121Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  Is it morally right to try to have a certain p...   \n1  What is the difference between marriage licens...   \n2  explain function calling. how would you call a...   \n3  How can I create a test set for a very rare ca...   \n4  What is the best way to travel from Tel-Aviv t...   \n\n                                          response_a  \\\n0  The question of whether it is morally right to...   \n1  A marriage license is a legal document that al...   \n2  Function calling is the process of invoking or...   \n3  Creating a test set for a very rare category c...   \n4  The best way to travel from Tel Aviv to Jerusa...   \n\n                                          response_b  winner_model_a  \\\n0  As an AI, I don't have personal beliefs or opi...               1   \n1  A marriage license and a marriage certificate ...               0   \n2  Function calling is the process of invoking a ...               0   \n3  When building a classifier for a very rare cat...               1   \n4  The best way to travel from Tel-Aviv to Jerusa...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>explain function calling. how would you call a...</td>\n      <td>Function calling is the process of invoking or...</td>\n      <td>Function calling is the process of invoking a ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>How can I create a test set for a very rare ca...</td>\n      <td>Creating a test set for a very rare category c...</td>\n      <td>When building a classifier for a very rare cat...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>What is the best way to travel from Tel-Aviv t...</td>\n      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"train['text'] = 'User prompt: ' + train['prompt'] + '\\n\\nModel A: \\n' + train['response_a'] + '\\n\\nModel B:\\n' + train['response_b']\nprint(train['text'][4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T22:23:27.208060Z","iopub.execute_input":"2025-05-05T22:23:27.208416Z","iopub.status.idle":"2025-05-05T22:23:27.467312Z","shell.execute_reply.started":"2025-05-05T22:23:27.208388Z","shell.execute_reply":"2025-05-05T22:23:27.462630Z"}},"outputs":[{"name":"stdout","text":"User prompt: What is the best way to travel from Tel-Aviv to Jerusalem? Car? Bus? Plane?\n\nModel A: \nThe best way to travel from Tel Aviv to Jerusalem depends on your personal preference and the availability of transportation options. All the options you have mentioned are valid options, but here are some details to help you make your decision:\\n\\n*   By car: Traveling by car is the quickest way to get from Tel Aviv to Jerusalem, as the distance between the two cities is only about 60 kilometers (37 miles). It takes around 45 minutes to drive from Tel Aviv to Jerusalem by car, depending on the traffic.\\n*   By bus: There are several bus lines that run from Tel Aviv to Jerusalem, and the journey takes around 1 hour and 30 minutes by bus. The buses are comfortable and reliable, and they offer a scenic view of the beautiful Israeli countryside.\\n*   By plane: There are no direct flights from Tel Aviv to Jerusalem, so you need to take a flight from Tel Aviv's Ben Gurion International Airport to Jerusalem's Sde Dov Airport. The flight takes around 30 minutes, and it is a convenient option if you are short on time.\\n\\nIn general, if you have enough time and don't mind taking the bus, I recommend that option. The buses are comfortable, reliable, and offer a scenic view of the countryside. If you are short on time and don't want to take a bus, a flight is another option to consider.\n\nModel B:\nThe best way to travel from Tel-Aviv to Jerusalem depends on your personal preferences and circumstances. \\n\\nIf you have a car and enjoy driving, then driving from Tel-Aviv to Jerusalem is an option. However, parking in Jerusalem can be challenging and expensive.\\n\\nIf you prefer to use public transportation, there are several bus lines that operate between Tel-Aviv and Jerusalem. Some of the most popular bus companies include Egged and Dan. The bus ride typically takes about an hour, depending on traffic.\\n\\nTaking a plane is not a recommended option since Tel-Aviv and Jerusalem are relatively close cities, and there are no airports in Jerusalem. \\n\\nIn summary, taking a bus is the most commonly used and convenient way to travel from Tel-Aviv to Jerusalem.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"train = train[:int(len(train) * 0.5)]\ntrain.loc[:, 'token_count'] = get_token_lengths(train['text'])\n\n#prepare label for model\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)\ndisplay(train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T22:30:07.314820Z","iopub.execute_input":"2025-05-05T22:30:07.315143Z","iopub.status.idle":"2025-05-05T22:30:07.361418Z","shell.execute_reply.started":"2025-05-05T22:30:07.315119Z","shell.execute_reply":"2025-05-05T22:30:07.358215Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train \u001b[38;5;241m=\u001b[39m train[:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)]\n\u001b[0;32m----> 2\u001b[0m train\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_lengths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#prepare label for model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(train[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner_model_a\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner_model_b\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner_tie\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36mget_token_lengths\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_token_lengths\u001b[39m(texts):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#tokenize and receive input_ids for reach text\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(texts\u001b[38;5;241m.\u001b[39mtolist(), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m input_ids]\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}],"execution_count":36},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T22:31:50.192641Z","iopub.execute_input":"2025-05-05T22:31:50.192996Z","iopub.status.idle":"2025-05-05T22:31:50.233481Z","shell.execute_reply.started":"2025-05-05T22:31:50.192968Z","shell.execute_reply":"2025-05-05T22:31:50.226346Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_10/35121072.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'label'"],"ename":"AttributeError","evalue":"'DataFrame' object has no attribute 'label'","output_type":"error"}],"execution_count":37},{"cell_type":"code","source":"display(train['token_count'].describe().to_frame().astype(int))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.percentile(train['token_count'], 90)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Tokenize Data\ntokens = tokenizer(\n    train['text'].to_list(),\n    padding ='max_length',\n    max_lenth=CFD.MAX_LENGTH,\n    truncation=True\n    return_tensors='np'\n)text\n\n#Input IDs are the token IDs\nINPUT_IDS = tokens['input_ids']\n#Attetion masks to ignore Padding Tokens\nATTENTION_MASKS = tokens['attention_mask']\n#Label of Texts\nLABELS = train[['winner_model_a','winner_model_b', 'winner_tie']].values\n\nprint(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\nprint(f'LABELS shape: {LABELS.shape}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_dataset(batch_size):\n    N_SAMPLES = LABELS.shape[0]\n    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n    while True:\n        #Shuffle indices\n        np.random.shuffle(IDXS)\n        #Iterate over all indices Once\n        for idxs in IDXS.reshape(-1, batch_size):\n            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n            attention_mask = torch.tensor(ATTENTION_MASKS[]idxs).to(DEVICE)\n            labels = torch.tensor(LABELS[idxs]).to(DEVICE)\n\n            #shard over TPU nodes if applicable (need to define mesh appropriately)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n\n            yield input_ids, attention_mask, labels\n\nTRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model for classification with 3 target label\nbase_model = LlamaForSequenceClassification.from_pretrained(\n    CFG.MODEL_NAME,\n    num_labels=CFG.NUM_LABELS,\n    torch_dtype=torch.bfloat16)\n\nbase_model.config.pretraining_tp = 1 \n\n# Assign Padding TOKEN\nbase_model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Low-Rank Adaptation[LORA]**","metadata":{}},{"cell_type":"code","source":"lora_congig = LoraConfig(\n    r=CFG.LORA_RANK, #Dimension of low-rank parameters\n    lora_alpha = CFG.LORA_ALPHA, #scaling factor for lora activations vs pre-trained weight actiations\n    lora_dropout = CFG.DROPOUT,\n    bias='none',\n    inference_mode = False\n    task_type=TaskType=SEQ_CLS\n    target_modules=CFG.LORA_MODULES\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Create lora model\nodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Number of TPU Nodes\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n\n# distribute model\npartition_module(model, mesh)\n\nprint(f'num_devices: {num_devices}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Verifiy trainable layers\nMODEL_LAYERS_ROWS = []\nTRAINABLE_PARAMS = []\nN_TRAINABLE_PARAMS = 0\n\n\nfor name, param in model.named_parameters():\n    #Layer parameter count\n    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n    #Only Trainable layers\n    if param.requires_grad:\n        #Add layter info\n        MODEL_LAYERS_ROW.append({\n            'param': n_parameters,\n            'name': name,\n            'dtype': param.data.type,\n        })\n        #Append Trainable parameter\n        TRAINABLE_PARAMS.append({'params':param})\n        #Add Number Of Trainable parameters\n        N_TRAINABLE_PARAMS += n_parameters\n\ndisplay(pd.DataFrame(MODEL_LAYERS_ROWS))\n\n\nprint(f\"\"\"\n===============================\nN_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\nN_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n===============================\n\"\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Trainig**","metadata":{}},{"cell_type":"code","source":"#LR & Optimizer\nN_SAMPLES = len(train)\nSTEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n\n#Cosine  Learning Rate with Warmup\nlr_scheduler = transformers.get_cosine_schedule_with_warmup(\n    optimizer=OPTIMIZER,\n    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS\n)\n\n\nprint(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:28:11.349050Z","iopub.execute_input":"2025-05-06T13:28:11.349389Z","iopub.status.idle":"2025-05-06T13:28:11.357793Z","shell.execute_reply.started":"2025-05-06T13:28:11.349364Z","shell.execute_reply":"2025-05-06T13:28:11.354216Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Set the data type for the optimizer's state (e.g., momentum buffers)\nfor state in OPTIMIZER.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.tensor) and state[k].dtype is not torch.float32:\n            state[v] = v.to(dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids, attention_mask, labels = next(TRAIN_DATASET)\n\nprint(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\nprint(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\nprint(f'labels shape: {labels.shape}, dtype: {labels.dtype}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n#Dummy Prediction\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\nprint(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype} ')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Put model in train mode\nmodel.train()\n\n#Loss function, cross entropy\nLOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:55:17.548446Z","iopub.execute_input":"2025-05-06T13:55:17.548756Z","iopub.status.idle":"2025-05-06T13:55:17.576210Z","shell.execute_reply.started":"2025-05-06T13:55:17.548731Z","shell.execute_reply":"2025-05-06T13:55:17.571338Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Put model in train mode\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Loss function, cross entropy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m LOSS_FN \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"st = time()\nwarnings.filterwarnings('error')\nMETRICS = {\n    'loss': [],\n    'accuracy': {'y_true':[], 'y_pred':[]}\n}\n\nfor epoch in tqdm(range(CFG.NUM_EPOCHS)):\n    ste=time()\n    for step in range(STEPS_PER_EPOCH):\n        #Zero Out Gradients\n        OPTIMIZER.zero_grad()\n        #Get Batch\n        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        #Logits float32\n        logits=outputs.logits.to(dtype=torch.float32)\n        #Backward pass\n        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n        loss.backward()\n\n        #optimizer step\n        OPTIMIZER.step()\n        xm.mark_step()\n\n        #Update leraning rate schedulerç\n        lr_scheduler.step()\n\n\n        #Update metrics and progress bar\n        METRICS['loss'].append(float(loss))\n        METRICS['ACCURACY']['y_true'] += labels.squeeze().tolist()\n        METRICS['ACCURACY']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()\n\n        if (steps +1) % 200 == 0:\n            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\ METRICS['accuracy']['y_pred']))\n\n            lr = OPTIMIZER.param_groups[0]['lr']\n            print(f'{epoch+1:02}/{CFG.NUM_EPOCH:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics} ', end='')\n            print(f'\\nSteps per epoch:{step+1} complete | Time elapsed: {time()-st} ')\n            \n        print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )\n\n        #If stopped, and to continue training in future on tpu we save model and optimizer\n        xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')\n        xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')\n\n        print(f'Model saved at epoch {epoch+1} | Elapsed time: {time() - st}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplt.plot(METRICS['loss'])\nplt.xlabel('Step per epoch')\nplt.ylabel('Loss')\nplt.title('Loss Plot step per epoch')    \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.cpu()\ntorch.save(dict([(k, v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}